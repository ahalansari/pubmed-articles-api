<?xml version="1.0"?>
<Container version="2">
  <Name>pubmed-articles-api</Name>
  <Repository>ghcr.io/ahalansari/pubmed-articles-api:latest</Repository>
  <Registry>https://ghcr.io</Registry>
  <Branch>
    <Tag>latest</Tag>
    <TagDescription>Latest stable release</TagDescription>
  </Branch>
  <Network>bridge</Network>
  <MyIP/>
  <Shell>bash</Shell>
  <Privileged>false</Privileged>
  <Support>https://github.com/ahalansari/pubmed-articles-api/issues</Support>
  <Project>https://github.com/ahalansari/pubmed-articles-api</Project>
  <Overview>
    PubMed Articles API - A Flask-based REST API for searching PubMed, retrieving open-access articles from PubMed Central, and generating AI-powered clinical summaries using LLM backends (LM Studio or vLLM).
    
    Features:
    - Search 35+ million PubMed citations
    - Retrieve open-access full-text from PMC
    - LLM-powered search term optimization
    - AI clinical summaries with chunking support
    - Demographic-aware filtering
    
    Requires an LLM backend (LM Studio or vLLM) for summarization features.
  </Overview>
  <Category>Productivity: Tools:</Category>
  <WebUI>http://[IP]:[PORT:8000]/api/v1/docs</WebUI>
  <TemplateURL>https://raw.githubusercontent.com/ahalansari/pubmed-articles-api/main/pubmed-articles-api.xml</TemplateURL>
  <Icon>https://raw.githubusercontent.com/ahalansari/pubmed-articles-api/main/icon.png</Icon>
  <ExtraParams>--restart=unless-stopped</ExtraParams>
  <PostArgs/>
  <CPUset/>
  <DateInstalled/>
  <DonateText/>
  <DonateLink/>
  <Requires>
    LLM Backend (LM Studio or vLLM) for AI summarization features.
  </Requires>
  <Config Name="API Port" Target="8000" Default="8000" Mode="tcp" Description="Port for the API server" Type="Port" Display="always" Required="true" Mask="false">8000</Config>
  <Config Name="API Key" Target="API_KEY" Default="" Mode="" Description="API key for authentication. Generate with: python generate_api_key.py quick" Type="Variable" Display="always" Required="true" Mask="true"/>
  <Config Name="NCBI API Key" Target="NCBI_API_KEY" Default="" Mode="" Description="Optional NCBI API key for higher rate limits (10 req/sec vs 3 req/sec). Get free at: https://www.ncbi.nlm.nih.gov/account/settings/" Type="Variable" Display="always" Required="false" Mask="true"/>
  <Config Name="NCBI Email" Target="NCBI_EMAIL" Default="" Mode="" Description="Email for NCBI API identification (recommended)" Type="Variable" Display="always" Required="false" Mask="false"/>
  <Config Name="LLM Backend" Target="LLM_BACKEND" Default="lmstudio" Mode="" Description="LLM backend: 'lmstudio' or 'vllm'" Type="Variable" Display="always" Required="true" Mask="false">lmstudio</Config>
  <Config Name="LM Studio URL" Target="LM_STUDIO_BASE_URL" Default="http://172.17.0.1:1234/v1" Mode="" Description="LM Studio API URL (use Docker host IP or container name)" Type="Variable" Display="always" Required="false" Mask="false">http://172.17.0.1:1234/v1</Config>
  <Config Name="LM Studio Model" Target="LM_STUDIO_MODEL" Default="default" Mode="" Description="LM Studio model name" Type="Variable" Display="always" Required="false" Mask="false">default</Config>
  <Config Name="LM Studio Context Window" Target="LM_STUDIO_CONTEXT_WINDOW" Default="8192" Mode="" Description="Context window size for chunking" Type="Variable" Display="advanced" Required="false" Mask="false">8192</Config>
  <Config Name="vLLM URL" Target="VLLM_LLM_BASE_URL" Default="http://172.17.0.1:8080/v1" Mode="" Description="vLLM API URL (if using vLLM backend)" Type="Variable" Display="advanced" Required="false" Mask="false"/>
  <Config Name="vLLM Model" Target="VLLM_LLM_MODEL" Default="meta-llama/Meta-Llama-3-8B-Instruct" Mode="" Description="vLLM model name" Type="Variable" Display="advanced" Required="false" Mask="false"/>
  <Config Name="vLLM API Key" Target="VLLM_API_KEY" Default="EMPTY" Mode="" Description="vLLM API key (usually EMPTY)" Type="Variable" Display="advanced" Required="false" Mask="false">EMPTY</Config>
  <Config Name="vLLM Max Tokens" Target="VLLM_MAX_TOKENS" Default="2048" Mode="" Description="Maximum response tokens for vLLM" Type="Variable" Display="advanced" Required="false" Mask="false">2048</Config>
  <Config Name="vLLM Context Window" Target="VLLM_CONTEXT_WINDOW" Default="8192" Mode="" Description="vLLM context window size for chunking" Type="Variable" Display="advanced" Required="false" Mask="false">8192</Config>
</Container>

